

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>TTS papers &mdash; zh794390558.github.io 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="CRF(Conditional Random Fields)" href="../../text_process/crf.html" />
    <link rel="prev" title="Speech Synthesis" href="../speech_synthesis.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> zh794390558.github.io
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../introduction.html">Hui Zhang</a></li>
</ul>
<p class="caption"><span class="caption-text">Speech</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../asr/chinese_syllable.html">chinese syllable</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../asr/alignment.html">Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../asr/edit-distance-papers/README.html">ASR Edit-distance as objective function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../decode/decoding.html">Decoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lm/ppl.html"><strong>语言模型评价指标Perplexity</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lm/ngram_lm.html">Ngram LM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../README.html">TTS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_synthesis.html">Speech Synthesis</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TTS papers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#papers">Papers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multi-speaker-papers">Multi-Speaker Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attention">Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vocoders">Vocoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#from-the-internet-blogs-videos-etc">From the Internet (Blogs, Videos etc)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#videos">Videos</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#paper-discussion">Paper Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#talks">Talks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#general">General</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#blogs">Blogs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../text_process/crf.html">CRF(Conditional Random Fields)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../text_process/asr_text_backend.html">ASR Text Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../text_process/tts_text_frontend.html">Text Front End</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vad/vad.html">VAD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spk/README.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../separation/README.html">Speech Separation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../io/README.html">Speech I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../io/praat_textgrid.html">Praat and TextGrid</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/training/speech_features.html">Speech Features</a></li>
</ul>
<p class="caption"><span class="caption-text">Coding</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/algorithm.html">Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/tutorials.html">Coding Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/cpp.html">library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/linear_algebra.html">线性代数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html">Eigen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#id11">矩阵和向量的运算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#array">Array类和元素级操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#id27">块操作</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#id32">高级初始化方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#id37">归约、迭代器和广播</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/eigen.html#map">原生缓存的接口：Map类</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/cuda.html">CUDA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/hpc.html">HPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/aot/python_to_cpp.html">Python code to Cpp</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../development/tfrt.html">TFRT: A New TensorFlow Runtime </a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/docker.html">Develop with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/deltann_compile.html">Tensorflow compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/adding_op.html">Adding Tensorflow Op</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/serving.html">Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/tensorrt.html">TensorRT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/model_optimization.html">Model Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/contributing.html">Contributing Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/version.html">Version</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/linux.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/ffmpeg.html">FFPMEG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../development/tools.html">Useful Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../product/product_value.html">用户体验、用户价值和产品价值</a></li>
</ul>
<p class="caption"><span class="caption-text">Dataset</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">Dataset</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">zh794390558.github.io</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>TTS papers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../_sources/speech/tts/TTS-papers/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tts-papers">
<h1>TTS papers<a class="headerlink" href="#tts-papers" title="Permalink to this headline">¶</a></h1>
<p>https://github.com/erogol/TTS-papers.git</p>
<div class="section" id="papers">
<h2>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Merging Phoneme and Char representations: https://arxiv.org/pdf/1811.07240.pdf</p></li>
<li><p>Tacotron transfer learning : https://arxiv.org/pdf/1904.06508.pdf</p></li>
<li><p>phoneme timing from attention: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8683827</p></li>
<li><p>SEMI-SUPERVISED TRAINING FOR IMPROVING DATA EFFICIENCY IN END-TO-ENDSPEECH SYNTHESI - https://arxiv.org/pdf/1808.10128.pdf</p></li>
<li><p>Listening while Speaking: Speech Chain by Deep Learning - https://arxiv.org/pdf/1707.04879.pdf</p></li>
<li><p>GENERALIZED END-TO-END LOSS FOR SPEAKER VERIFICATION: https://arxiv.org/pdf/1710.10467.pdf</p></li>
<li><p>Es-Tacotron2: Multi-Task Tacotron 2 with Pre-Trained Estimated Network for Reducing the Over-Smoothness Problem: https://www.mdpi.com/2078-2489/10/4/131/pdf</p>
<ul>
<li><p>Against Over-Smoothness</p></li>
</ul>
</li>
<li><p>FastSpeech: https://arxiv.org/pdf/1905.09263.pdf</p></li>
<li><p>Learning singing from speech: https://arxiv.org/pdf/1912.10128.pdf</p></li>
<li><p>TTS-GAN: https://arxiv.org/pdf/1909.11646.pdf</p>
<ul>
<li><p>they use duration and linguistic features for en2en TTS.</p></li>
<li><p>Close to WaveNet performance.</p></li>
</ul>
</li>
<li><p>DurIAN: https://arxiv.org/pdf/1909.01700.pdf</p>
<ul>
<li><p>Duration aware Tacotron</p></li>
</ul>
</li>
<li><p>MelNet: https://arxiv.org/abs/1906.01083</p></li>
<li><p>AlignTTS: https://arxiv.org/pdf/2003.01950.pdf</p></li>
<li><p>Unsupervised Speech Decomposition via Triple Information Bottleneck</p>
<ul>
<li><p>https://arxiv.org/pdf/2004.11284.pdf</p></li>
<li><p>https://anonymous0818.github.io/</p></li>
</ul>
</li>
<li><p>FlowTron: https://arxiv.org/pdf/2005.05957.pdf</p>
<ul>
<li><p>Inverse Autoregresive Flow on Tacotron like architecture</p></li>
<li><p>WaveGlow as vocoder.</p></li>
<li><p>Speech style embedding with Mixture of Gaussian model.</p></li>
<li><p>Model is large and havier than vanilla Tacotron</p></li>
<li><p>MOS values are slighly better than public Tacotron implementation.</p></li>
</ul>
</li>
<li><p>Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention : https://arxiv.org/pdf/1710.08969.pdf</p></li>
</ul>
<details>
<summary> End-to-End Adversarial Text-to-Speech: http://arxiv.org/abs/2006.03575 (Click to Expand)</summary> <ul class="simple">
<li><p>end2end feed-forward TTS learning.</p></li>
<li><p>Character alignment has been done with a separate aligner module.</p></li>
<li><p>The aligner predicts length of each character.
- The center location of a char is found wrt the total length of the previous characters.
- Char positions are interpolated with a Gaussian window wrt the real audio length.</p>
<ul>
<li><p>audio output is computed in mu-law domain. (I don't have a reasoning for this)</p></li>
<li><p>use only 2 secs audio windows for traning.</p></li>
<li><p>GAN-TTS generator is used to produce audio signal.</p></li>
<li><p>RWD is used as a audio level discriminator.</p></li>
<li><p>MelD: They use BigGAN-deep architecture as spectrogram level discriminator regading the problem as image reconstruction.</p></li>
<li><p>Spectrogram loss</p>
<ul>
<li><p>Using only adversarial feed-back is not enough to learn the char alignments. They use a spectrogram loss b/w predicted spectrograms and ground-truth specs.</p></li>
<li><p>Note that model predicts audio signals. Spectrograms above are computed from the generated audio.</p></li>
<li><p>Dynamic Time Wraping is used to compute a minimal-cost alignment b/w generated spectrograms and ground-truth.</p></li>
<li><p>It involves a dynamic programming approach to find a minimal-cost alignment.</p></li>
</ul>
</li>
<li><p>Aligner length loss is used to penalize the aligner for predicting different than the real audio length.</p></li>
<li><p>They train the model with multi speaker dataset but report results on the best performing speaker.</p></li>
<li><p>Ablation Study importance of each component: (LengthLoss and SpectrogramLoss) &gt; RWD &gt; MelD &gt; Phonemes &gt; MultiSpeakerDataset.</p></li>
<li><p>My 2 cents: It is a feed forward model which provides end-2-end speech synthesis with no need to train a separate vocoder model. However, it is very complicated model with a lot of hyperparameters and implementation details. Also the final result is not close to the state of the art. I think we need to find specific algorithms for learning character alignments which would reduce the need of tunning a combination of different algorithms.
<img src="https://user-images.githubusercontent.com/1402048/84696449-d25e6b80-af4c-11ea-8b3a-66ede19124b0.png" width="50%"></p></li>
</ul>
</li>
</ul>
</details><details>
<summary> Fast Speech2: http://arxiv.org/abs/2006.04558 (Click to Expand)</summary> <ul class="simple">
<li><p>Use phoneme durations generated by <a class="reference external" href="https://montreal-forced-aligner.readthedocs.io/en/latest/introduction.html">MFA</a> as labels to train a length regulator.</p></li>
<li><p>Thay use frame level F0 and L2 spectrogram norms (Variance Information) as additional features.</p></li>
<li><p>Variance predictor module predicts the variance information at inference time.</p></li>
<li><p>Ablation study result improvements: model &lt; model + L2_norm &lt; model + L2_norm + F0
<img alt="image" src="https://user-images.githubusercontent.com/1402048/84696094-3c2a4580-af4c-11ea-8de3-4e918d651cd4.png" /></p></li>
</ul>
</details><details>
<summary> Glow-TTS: https://arxiv.org/pdf/2005.11129.pdf (Click to Expand)</summary> <ul class="simple">
<li><p>Use Monotonic Alignment Search to learn the alignment b/w text and spectrogram</p></li>
<li><p>This alignment is used to train a Duration Predictor to be used at inference.</p></li>
<li><p>Encoder maps each character to a Gaussian Distribution.</p></li>
<li><p>Decoder maps each spectrogram frame to a latent vector using Normalizing Flow (Glow Layers)</p></li>
<li><p>Encoder and Decoder outputs are aligned with MAS.</p></li>
<li><p>At each iteration first the most probable alignment is found by MAS and this alignment is used to update mode parameters.</p></li>
<li><p>A duration predictor is trained to predict the number of spectrogram frames for each character.</p></li>
<li><p>At inference only the duration predictor is used instead of MAS</p></li>
<li><p>Encoder has the architecture of the TTS transformer with 2 updates</p></li>
<li><p>Instead of absolute positional encoding, they use realtive positional encoding.</p></li>
<li><p>They also use a residual connection for the Encoder Prenet.</p></li>
<li><p>Decoder has the same architecture as the Glow model.</p></li>
<li><p>They train both single and multi-speaker model.</p></li>
<li><p>It is showed experimentally, Glow-TTS is more robust against long sentences compared to original Tacotron2</p></li>
<li><p>15x faster than Tacotron2 at inference</p></li>
<li><p>My 2 cents: Their samples sound not as natural as Tacotron. I believe normal attention models still generate more natural speech since the attention learns to map characters to model outputs directly. However, using Glow-TTS might be a good alternative for hard datasets.</p></li>
<li><p>Samples: https://github.com/jaywalnut310/glow-tts</p></li>
<li><p>Repository: https://github.com/jaywalnut310/glow-tts
<img alt="image" src="https://user-images.githubusercontent.com/1402048/85527284-06035a80-b60b-11ea-8165-b2f3e841f77f.png" /></p></li>
</ul>
</details><div class="section" id="multi-speaker-papers">
<h3>Multi-Speaker Papers<a class="headerlink" href="#multi-speaker-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Training Multi-Speaker Neural Text-to-Speech Systems using Speaker-Imbalanced Speech Corpora - https://arxiv.org/abs/1904.00771</p></li>
<li><p>Deep Voice 2 - https://papers.nips.cc/paper/6889-deep-voice-2-multi-speaker-neural-text-to-speech.pdf</p></li>
<li><p>Sample Efficient Adaptive TTS - https://openreview.net/pdf?id=rkzjUoAcFX</p>
<ul>
<li><p>WaveNet + Speaker Embedding approach</p></li>
</ul>
</li>
<li><p>Voice Loop - https://arxiv.org/abs/1707.06588</p></li>
<li><p>MODELING MULTI-SPEAKER LATENT SPACE TO IMPROVE NEURAL TTS QUICK ENROLLING NEW SPEAKER AND ENHANCING PREMIUM VOICE - https://arxiv.org/pdf/1812.05253.pdf</p></li>
<li><p>Transfer learning from speaker verification to multispeaker text-to-speech synthesis - https://arxiv.org/pdf/1806.04558.pdf</p></li>
<li><p>Fitting new speakers based on a short untranscribed sample - https://arxiv.org/pdf/1802.06984.pdf</p></li>
<li><p>Generalized end-to-end loss for speaker verification</p></li>
</ul>
</div>
<div class="section" id="attention">
<h3>Attention<a class="headerlink" href="#attention" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>LOCATION-RELATIVE ATTENTION MECHANISMS FOR ROBUST LONG-FORMSPEECH SYNTHESIS : https://arxiv.org/pdf/1910.10288.pdf</p></li>
</ul>
</div>
<div class="section" id="vocoders">
<h3>Vocoders<a class="headerlink" href="#vocoders" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>MelGAN: https://arxiv.org/pdf/1910.06711.pdf</p></li>
<li><p>ParallelWaveGAN: https://arxiv.org/pdf/1910.11480.pdf</p>
<ul>
<li><p>Multi scale STFT loss</p></li>
<li><p>~1M model parameters (very small)</p></li>
<li><p>Slightly worse than WaveRNN</p></li>
</ul>
</li>
<li><p>Improving FFTNEt</p>
<ul>
<li><p>https://www.okamotocamera.com/slt_2018.pdfF</p></li>
<li><p>https://www.okamotocamera.com/slt_2018.pdf</p></li>
</ul>
</li>
<li><p>FFTnet</p>
<ul>
<li><p>https://gfx.cs.princeton.edu/pubs/Jin_2018_FAR/clips/clips.php</p></li>
<li><p>https://gfx.cs.princeton.edu/pubs/Jin_2018_FAR/fftnet-jin2018.pdf</p></li>
</ul>
</li>
<li><p>SPEECH WAVEFORM RECONSTRUCTION USING CONVOLUTIONAL NEURALNETWORKS WITH NOISE AND PERIODIC INPUTS</p>
<ul>
<li><p>150.162.46.34:8080/icassp2019/ICASSP2019/pdfs/0007045.pdf</p></li>
</ul>
</li>
<li><p>Towards Achieveing Robust Universal Vocoding</p>
<ul>
<li><p>https://arxiv.org/pdf/1811.06292.pdf</p></li>
</ul>
</li>
<li><p>LPCNet</p>
<ul>
<li><p>https://arxiv.org/pdf/1810.11846.pdf</p></li>
<li><p>https://arxiv.org/pdf/2001.11686.pdf</p></li>
</ul>
</li>
<li><p>ExciteNet</p>
<ul>
<li><p>https://arxiv.org/pdf/1811.04769v3.pdf</p></li>
</ul>
</li>
<li><p>GELP: GAN-Excited Linear Prediction for Speech Synthesis fromMel-spectrogram</p>
<ul>
<li><p>https://arxiv.org/pdf/1904.03976v3.pdf</p></li>
</ul>
</li>
<li><p>High Fidelity Speech Synthesis with Adversarial Networks: https://arxiv.org/abs/1909.11646</p>
<ul>
<li><p>GAN-TTS, end-to-end speech synthesis</p></li>
<li><p>Uses duration and linguistic features</p></li>
<li><p>Duration and acoustic features are predicted by additional models.</p></li>
<li><p>Random Window Discriminator: Ingest not the whole Voice sample but random
windows.</p></li>
<li><p>Multiple RWDs. Some conditional and some unconditional. (conditioned on
input features)</p></li>
<li><p>Punchline: Use randomly sampled windows with different window sizes for D.</p></li>
<li><p>Shared results sounds mechanical that shows the limits of non-neural
acoustic features.</p></li>
</ul>
</li>
<li><p>Multi-Band MelGAN: https://arxiv.org/abs/2005.05106</p>
<ul>
<li><p>Use PWGAN losses instead of feature-matching loss.</p></li>
<li><p>Using a larger receptive field boosts model performance significantly.</p></li>
<li><p>Generator pretraining for 200k iters.</p></li>
<li><p>Multi-Band voice signal prediction. The output is summation of 4 different
band predictions with PQMF synthesis filters.</p></li>
<li><p>Multi-band model has 1.9m parameters (quite small).</p></li>
<li><p>Claimed to be 7x faster than MelGAN</p></li>
<li><p>On a Chinese dataset: MOS 4.22</p></li>
</ul>
</li>
<li><p>WaveGLow: https://arxiv.org/abs/1811.00002</p>
<ul>
<li><p>Very large model (268M parameters)</p></li>
<li><p>Hard to train since on 12GB GPU it can only takes batch size 1.</p></li>
<li><p>Real-time inference due to the use of convolutions.</p></li>
<li><p>Based on Invertable Normalizing Flow. (Great tutorial https://blog.evjang.com/2018/01/nf1.html
)</p></li>
<li><p>Model learns and invetible mapping of audio samples to mel-spectrograms with Max Likelihood loss.</p></li>
<li><p>In inference network runs in reverse direction and give mel-specs are converted to audio samples.</p></li>
<li><p>Training has been done using 8 Nvidia V100 with 32GB ram, batch size 24. (Expensive)</p></li>
</ul>
</li>
<li><p>SqueezeWave: https://arxiv.org/pdf/2001.05685.pdf , code: https://github.com/tianrengao/SqueezeWave</p>
<ul>
<li><p>~5-13x faster than real-time</p></li>
<li><p>WaveGlow redanduncies: Long audio samples, upsamples mel-specs, large channel dimensions in WN function.</p></li>
<li><p>Fixes: More but shorter audio samples as input,  (L=2000, C=8 vs L=64, C=256)</p></li>
<li><p>L=64 matchs the mel-spec resolution so no upsampling necessary.</p></li>
<li><p>Use depth-wise separable convolutions in WN modules.</p></li>
<li><p>Use regular convolution instead of dilated since audio samples are shorter.</p></li>
<li><p>Do not split module outputs to residual and network output, assuming these vectors are almost identical.</p></li>
<li><p>Training has been done using Titan RTX 24GB batch size 96 for 600k iterations.</p></li>
<li><p>MOS on LJSpeech: WaveGLow - 4.57, SqueezeWave (L=128 C=256) - 4.07 and SqueezeWave (L=64 C=256) - 3.77</p></li>
<li><p>Smallest model has 21K samples per second on Raspi3.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="from-the-internet-blogs-videos-etc">
<h2>From the Internet (Blogs, Videos etc)<a class="headerlink" href="#from-the-internet-blogs-videos-etc" title="Permalink to this headline">¶</a></h2>
<div class="section" id="videos">
<h3>Videos<a class="headerlink" href="#videos" title="Permalink to this headline">¶</a></h3>
<div class="section" id="paper-discussion">
<h4>Paper Discussion<a class="headerlink" href="#paper-discussion" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Tacotron 2 : https://www.youtube.com/watch?v=2iarxxm-v9w</p></li>
</ul>
</div>
<div class="section" id="talks">
<h4>Talks<a class="headerlink" href="#talks" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>End-to-End Text-to-Speech Synthesis, Part 1 : https://www.youtube.com/watch?v=RNKrq26Z0ZQ</p></li>
<li><p>Speech synthesis from neural decoding of spoken sentences | AISC : https://www.youtube.com/watch?v=MNDtMDPmnMo</p></li>
<li><p>Generative Text-to-Speech Synthesis : https://www.youtube.com/watch?v=j4mVEAnKiNg</p></li>
<li><p>SPEECH SYNTHESIS FOR THE GAMING INDUSTRY : https://www.youtube.com/watch?v=aOHAYe4A-2Q</p></li>
</ul>
</div>
<div class="section" id="general">
<h4>General<a class="headerlink" href="#general" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Modern Text-to-Speech Systems Review : https://www.youtube.com/watch?v=8rXLSc-ZcRY</p></li>
</ul>
</div>
</div>
<div class="section" id="blogs">
<h3>Blogs<a class="headerlink" href="#blogs" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Text to Speech Deep Learning Architectures : http://www.erogol.com/text-speech-deep-learning-architectures/</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../../text_process/crf.html" class="btn btn-neutral float-right" title="CRF(Conditional Random Fields)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../speech_synthesis.html" class="btn btn-neutral float-left" title="Speech Synthesis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Hui Zhang.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>